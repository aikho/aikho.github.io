<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Что автоматизирует AutoML? | AIkho.github.io</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="Что автоматизирует AutoML?" />
<meta name="author" content="Андрей Хобня" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="В мае 2017 года исследователи из Google Brain team представили AutoML – алгоритм, который позволяет автоматически создавать архитектуры нейронных сетей, используя обучение с подкреплением. Через несколько месяцев они успешно применили AutoML для создания нейросети NASNet. Многие высказывали мнение, что теперь создавать нейронные сети сможет каждый и не нужны будут специалисты в этой области. Однако, на данный момент создавать новые модели машинного обучения все так же сложно. Так что же сделали в Google Brain team? В этом нам помогут разобраться публикации авторов AutoML." />
<meta property="og:description" content="В мае 2017 года исследователи из Google Brain team представили AutoML – алгоритм, который позволяет автоматически создавать архитектуры нейронных сетей, используя обучение с подкреплением. Через несколько месяцев они успешно применили AutoML для создания нейросети NASNet. Многие высказывали мнение, что теперь создавать нейронные сети сможет каждый и не нужны будут специалисты в этой области. Однако, на данный момент создавать новые модели машинного обучения все так же сложно. Так что же сделали в Google Brain team? В этом нам помогут разобраться публикации авторов AutoML." />
<link rel="canonical" href="https://aikho.github.io/2018/05/01/what-does-automl-automate.html" />
<meta property="og:url" content="https://aikho.github.io/2018/05/01/what-does-automl-automate.html" />
<meta property="og:site_name" content="AIkho.github.io" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-05-01T18:20:00+03:00" />
<script type="application/ld+json">
{"description":"В мае 2017 года исследователи из Google Brain team представили AutoML – алгоритм, который позволяет автоматически создавать архитектуры нейронных сетей, используя обучение с подкреплением. Через несколько месяцев они успешно применили AutoML для создания нейросети NASNet. Многие высказывали мнение, что теперь создавать нейронные сети сможет каждый и не нужны будут специалисты в этой области. Однако, на данный момент создавать новые модели машинного обучения все так же сложно. Так что же сделали в Google Brain team? В этом нам помогут разобраться публикации авторов AutoML.","author":{"@type":"Person","name":"Андрей Хобня"},"@type":"BlogPosting","url":"https://aikho.github.io/2018/05/01/what-does-automl-automate.html","headline":"Что автоматизирует AutoML?","dateModified":"2018-05-01T18:20:00+03:00","datePublished":"2018-05-01T18:20:00+03:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://aikho.github.io/2018/05/01/what-does-automl-automate.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://aikho.github.io/feed.xml" title="AIkho.github.io" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">AIkho.github.io</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/rules/">Правила</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Что автоматизирует AutoML?</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-05-01T18:20:00+03:00" itemprop="datePublished">May 1, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>В мае 2017 года исследователи из Google Brain team представили <a href="https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html">AutoML</a> – алгоритм,
который позволяет автоматически создавать архитектуры нейронных сетей, используя обучение с подкреплением. Через несколько месяцев они успешно применили AutoML
для создания нейросети <a href="https://research.googleblog.com/2017/11/automl-for-large-scale-image.html">NASNet</a>. Многие высказывали мнение, что теперь создавать
нейронные сети сможет каждый и не нужны будут специалисты в этой области. Однако, на данный момент создавать новые модели машинного обучения все так же сложно.
Так что же сделали в Google Brain team? В этом нам помогут разобраться публикации авторов AutoML.</p>

<p>Сразу следует отметить, что сама идея метаобучения (обучения алгоритмов и моделей обучения, meta learning [<a href="#r1">1</a>])
не нова. Работы в данной области публикуют примерно с 1991 года. Обучение с подкреплением не единственный метод метаобучения. Рассматриваются также генетические
алгоритмы и другие подходы. Но давайте перейдем непосредственно к научной статье авторов AutoML [<a href="#r2">2</a>].</p>

<p>В разделе “3 Methods”, а именно в “3.1 Generate model descriptions with a controller recurrent neural network” авторы пишут:</p>

<blockquote>
  <p>In Neural Architecture Search, we use a controller to generate architectural hyperparameters of
neural networks. To be flexible, the controller is implemented as a recurrent neural network.</p>
</blockquote>

<p>Авторы обучают рекуррентную нейронную сеть, называемую контроллером, выводить гиперпараметры искомой архитектуры нейронной сети.</p>

<p>Далее в данном разделе приводится пример предсказания архитектуры нейронной сети, состоящей из одних сверточных слоев. В данном примере, рекуррентная сеть выводит последовательно
5 гиперпараметров для каждого слоя: высота фильтра, ширина фильтра, высота шага, ширина шага и количество фильтров.</p>

<blockquote>
  <p>Once the controller RNN finishes generating an architecture, a neural network with this architecture is built
and trained. At convergence, the accuracy of the network on a held-out validation set is recorded.
The parameters of the controller RNN, θc, are then optimized in order to maximize the expected
validation accuracy of the proposed architectures. In the next section, we will describe a policy
gradient method which we use to update parameters θc so that the controller RNN generates better
architectures over time.</p>
</blockquote>

<p>После того как контроллер сгенерировал гиперпараметры, строится архитектура новой нейронной сети, затем созданная сеть обучается, вычисляется ее точность.
Затем на основе данной обратной связи происходит обучение нейронной сети контроллера. Таким образом, контроллер обучается генерировать лучшие гиперпараметры архитектуры
при помощи обучения с подкреплением.</p>

<p>Т.е. <strong>AutoML не генерирует произвольные архитектуры</strong>. Чтобы использовать AutoML для своей задачи, необходимо определить нечто вроде <em>метаархитектуры</em> нейронной сети:
типы слоев, соединений, изменяемые гиперпараметры и т.д.</p>

<p>Но если AutoML не генерирует произвольные архитектуры, каким образом тогда она сгенерировала новый тип рекуррентных слоев? Посмотрим, что пишут об этом авторы в разделе 3.4:</p>

<blockquote>
  <p>The computations for basic RNN and LSTM cells can be generalized as a tree of steps that take xt
and ht−1 as inputs and produce ht as final output. The controller RNN needs to label each node in
the tree with a combination method (addition, elementwise multiplication, etc.) and an activation
function (tanh, sigmoid, etc.) to merge two inputs and produce one output. Two outputs are then
fed as inputs to the next node in the tree. To allow the controller RNN to select these methods and
functions, we index the nodes in the tree in an order so that the controller RNN can visit each node
one by one and label the needed hyperparameters.</p>
</blockquote>

<p>Авторы представили рекуррентную ячейку как дерево с тремя узлами. Каждому узлу соответствует функция комбинации и функция активации.
Функции выбираются из списка. Нейронная сеть контроллера учится выбирать функции, которые дадут лучший результат.</p>

<p>Хорошо, но каким образом удалось применить AutoML для создания NASNet, ведь архитектура NASNet достаточно сложна?
Изучим соответствующую работу [<a href="#r3">3</a>]. В разделе “3. Method” авторы пишут:</p>

<blockquote>
  <p>In our approach, the overall architectures of the convolutional
nets are manually predetermined. They are composed
of convolutional cells repeated many times where
each convolutional cell has the same architecture, but different
weights.</p>
</blockquote>

<p>Архитектуры сверточных сетей предопределены вручную. Они состоят из сверточных блоков, которые повторяются множество раз и имеют одинаковую архитектуру,
но различные веса. Это похоже на идею архитектуры Inception, которая тоже состоит из повторяющихся блоков.</p>

<p>В этом же разделе авторы пишут:</p>

<blockquote>
  <p>The main contribution of this work is the design of a
novel search space, such that the best architecture found
on the CIFAR-10 dataset would scale to larger, higher-resolution
image datasets across a range of computational
settings.</p>
</blockquote>

<p>Т.е. авторы тренируют AutoML на датасете CIFAR-10. Получают оптимальную архитектуру сверточного блока. Затем из полученных одинаковых блоков строят нейронную сеть
для датасета изображений с более высоким разрешением, т.е. ImageNet. Основной принцип сохраняется – сначала определяется метаархитектура и ее гиперпараметры,
затем осуществляется поиск оптимальных значений гиперпараметров при помощи машинного обучения с подкреплением.</p>

<p>Зачем оптимизировать архитектуру блока на CIFAR-10, а потом пытаться ее масштабировать на ImageNet? Почему бы не применить AutoML непосредственно к ImageNet?
Чтобы в этом разобраться, обратим внимание на данные из раздела “4. Experiments and Results”:</p>

<blockquote>
  <p>The method in this paper
uses 500 GPUs across 4 days resulting in 2,000 GPU-hours. The former
effort used Nvidia K40 GPUs, whereas the current efforts used faster
NVidia P100s.</p>
</blockquote>

<p>Карта NVidia P100s сейчас стоит примерно <a href="https://www.amazon.com/gp/offer-listing/B06WV7HFWV/?ie=UTF8&amp;condition=new">6’500$</a>.
Авторы использовали 500 штук. Получается, чтобы обучить AutoML за 4 дня даже на таком “игрушечном” датасете как CIFAR-10, необходимо
иметь доступ к оборудованию на 3’250’000$? На данный момент это так. А чтобы обучать AutoML на ImageNet, необходимо ресурсов
минимум на пару порядков больше.</p>

<h3 id="Выводы">Выводы</h3>

<p>AutoML – неплохой метод тюнинга архитектуры моделей машинного обучения. Однако, он не способен решить вашу задачу автоматически.
Для того, чтобы использовать AutoML для новой задачи, необходимо задать метаархитектуру модели (нейронной сети): типы слоев, соединений,
изменяемые гиперпараметры и т.д. Кроме того, применение AutoML требует использования большого количества вычислительных ресурсов.</p>

<p>Ссылки:</p>
<ol>
  <li><a name="r1"></a> <a href="https://en.wikipedia.org/wiki/Meta_learning_(computer_science)">Meta Learning</a></li>
  <li><a name="r2"></a> <a href="https://arxiv.org/abs/1611.01578">Neural Architecture Search with Reinforcement Learning (arXiv:1611.01578)</a></li>
  <li><a name="r3"></a> <a href="https://arxiv.org/abs/1707.07012">Learning Transferable Architectures for Scalable Image Recognition (arXiv:1707.07012)</a></li>
</ol>


  </div><div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = 'https://aikho.github.io/2018/05/01/what-does-automl-automate.html';
      this.page.identifier = 'https://aikho.github.io/2018/05/01/what-does-automl-automate.html';
    };

    (function() {
      var d = document, s = d.createElement('script');

      s.src = 'https://andrei-khobnia-blog.disqus.com/embed.js';

      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript><a class="u-url" href="/2018/05/01/what-does-automl-automate.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">AIkho.github.io</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Андрей Хобня</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Блог о машинном обучении и разработке интеллектуальных систем</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
